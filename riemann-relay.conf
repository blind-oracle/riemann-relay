# Global options
## Stats & Prometheus metrics
listenHTTP = "127.0.0.1:9999"

## How frequently to print stats in the logs
stats_interval = "5s"

# One or more inputs under 'input' subtree
[input.input1]
# Listen parameters can be IP:Port or Unix Socket path
# Set the parameter to empty or comment it out to disable it
# At least one of 'listen' or 'listenWS' have to be specified

## Riemann TCP Protobuf events
listen = "127.0.0.1:1234"

## WebSocket Riemann JSON events
listenWS = "127.0.0.1:12345"

# Read/Write timeout on incoming connections
timeout = "5s"

# A list of outputs where to send events received
# by this input. At least one output is required
outputs = [ "output1" ]

# One or more outputs under 'output' subtree
[output.output1]
# Output type, can be 'carbon' or 'riemann'
# Carbon is a plaintext one-metric-per-line
# Riemann is a Protobuf based output
type = "carbon"

# Algorithm to use when choosing targets
#
# Can be one of:
# - roundrobin: spread metrics over all targets evenly
# - broadcast: copy metrics to all targets
# - hash: use xxhash64 to choose a target to get consistent target mapping
#   See parameter 'hash_fields' below for hashing key
# - failover: send metrics to the first live target (left to right in the target list, see below)
algo = "hash"

# Send events to a random live target if the target selected by roundrobin/hash algorithms is dead.
# Obviously this makes no sense for broadcast/failover algorithms.
#
# If this is 'false' then the events for the down targets will be accumulated in their buffers and,
# if the target is down for an extended period of time, new events will be discarded.
#
# Be aware that the status of the target is not be reflected instantly and some small number of metrics
# might anyway go to target's buffer while it's still considered alive.
algo_failover = false

# List of Riemann event fields to use as a hashing key, required if the 'algo' is 'hash'
# These are evaluated left-to-right, although for hashing purposes it does not matter.
#
# Possible values:
# - state
# - service
# - host
# - description
#
# These directly map to corresponding fields in the event Protobuf.
#
# Additionally you can use:
# - tag:name
#   If there's a tag 'name' in the Event then 'name' will be used as value
# - attr:name
#   If there's an attribute 'name' in the Event then it's value will be used
# - custom:whatever
#   Just use the constant after the ':' as a value. Makes no sense for 'hash_fields',
#   but can be useful in 'carbon_fields'
#
# If there's no tag/attribute found by this name then it's ignored
hash_fields = [ "attr:foo", "host", "service", "tag:bar" ]

# List of Riemann event fields to use to form a Carbon metric name, required if output type is 'carbon'.
# The values of the fields are joined together with a '.' separator per Carbon specs.
#
# For syntax see 'hash_fields' parameter up there.
carbon_fields = [ "attr:prefix", "host", "service", "custom:foobar" ]

# Which Riemann value field to use when forming a Carbon metric.
# Riemann Protobuf has 3 value fields (int, float and double)
#
# Possible values:
# - int
# - float
# - double
# - any
#
# If you specify 'any' then all fields are checked (double -> int -> float)
# and first non-zero is used as a value. If all of them are zero then zero is used.
# This is probably the same that Riemann itself does according to docs.
#
# Value is converted to float64
carbon_value = "any"

# List of targets to send events to.
# Can be IP:Port or a Unix Socket path.
targets = [ "1.1.1.1:1234", "2.2.2.2:1234", "3.3.3.3:1234" ]

# Output metric buffer size *per target* (number of events)
#
# If the target is down then the metrics are accumulated in this buffer until
# the target is up again (unless 'algo_failover' is 'true')
# If the buffer is full then new metrics will be discarded.
buffer_size = 200000

# Number of metrics/events to send in one iteration.
#
# For Carbon this is a number of metrics in a single TCP write (larger batch -> fewer system calls)
# For Riemann this is a number of events sent in a single Protobuf message.
#
# If the amount of pending metrics is equal to this value then they're flushed immediately.
# If it's less then it will be flushed every 'batch_timeout'
batch_size = 50

# How frequently to flush batches to a target if it's less than 'batch_size'
batch_timeout = "1s"

# Delay between connection retries when the target is down
reconnect_interval = "1s"

# How long to wait for the connection to be established
connect_timeout = "1s"

# Read/Write timeout on outgoing connections
timeout = "5s"
